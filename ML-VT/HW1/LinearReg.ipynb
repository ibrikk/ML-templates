{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Proving that the Hat Matrix is Symmetric\n",
    "\n",
    "The **Hat matrix**  is used in linear regression to project observed values onto the space \n",
    "spanned by the predictor variables. \n",
    "The **Hat matrix** `H` is defined as:\n",
    "\n",
    "$H = X (X^T)^{-1} X^T$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "- X is the **design matrix**,\n",
    "- $ X^T $ is the **transpose** of `X`, and\n",
    "- $ (X^T X)^{-1} $ is the **inverse** of $ X^T X $.\n",
    "\n",
    "To prove that `H` is **symmetric**, we need to show that:\n",
    "\n",
    "\n",
    "$ H^T = H $\n",
    "\n",
    "\n",
    "Let's take the transpose of `H`:\n",
    "\n",
    "$ H^T = (X (X^T X)^{-1} X^T)^T $\n",
    "\n",
    "Using the **transpose rule** for matrix multiplication $ (ABC)^T = C^T B^T A^T $, we get:\n",
    "\n",
    "\n",
    "$ H^T = X (X^T X)^{-1} X^T $\n",
    "\n",
    "\n",
    "Since:\n",
    "\n",
    "1. $ (X^T)^T = X $ (transpose of a transpose is the original matrix).\n",
    "2. $ (X^T X)^{-1} $ is symmetric because the inverse of a symmetric matrix is also symmetric.\n",
    "\n",
    "We see that:\n",
    "\n",
    "$ H^T = H $\n",
    "\n",
    "Thus, the **Hat matrix is symmetric**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Proving that the Hat Matrix is Idempotent\n",
    "\n",
    "A matrix is **idempotent** if:\n",
    "\n",
    "$ H^2 = H $\n",
    "\n",
    "This means that if we apply the transformation represented by H twice, the result remains the same as applying it once.\n",
    "\n",
    "Expanding $ H^2 $:\n",
    "\n",
    "\n",
    "$ H^2 = H \\cdot H $\n",
    "\n",
    "Let's substitute `H`:\n",
    "\n",
    "\n",
    "$ H^2 = (X (X^T X)^{-1} X^T) \\cdot (X (X^T X)^{-1} X^T) $\n",
    "\n",
    "\n",
    "Let's rearrange the terms for better visibility:\n",
    "\n",
    "\n",
    "$ H^2 = X (X^T X)^{-1} X^T X (X^T X)^{-1} X^T $\n",
    "\n",
    "\n",
    "Since $ X^T X (X^T X)^{-1} = I $ (identity matrix), we simplify:\n",
    "\n",
    "\n",
    "$ H^2 = X (X^T X)^{-1} X^T = H $\n",
    "\n",
    "\n",
    "Thus, **the Hat matrix is idempotent**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear regression curves\n",
    "\n",
    "$  y = \\displaystyle\\sum_{i=0}^{n} a_i x^i  \\\\ $ \n",
    "Yes. This function is polynomial. It remains linear in the uknown parameters $ a_i $. It can be estimated using linear regression.\n",
    "\n",
    "$ y = ax + b \\cdot \\sin(x) + c \\cdot \\log(x) + d  \\\\ $ \n",
    "Yes, this trigonometric and logarithmic function can be estimated using linear regression. This equation remain linear in the unknown coefficients $ a, b, c, d $.\n",
    "\n",
    "$  y = ae^{(bx)}  \\\\ $ \n",
    "No. Exponential functions cannot be estimated using linear regression. \n",
    "\n",
    "$ y = ax + bx + c \\\\ $ \n",
    "Yes, this can be estimated using linear regression. This can be written as $ y = (a + b)x + c $, which is a simple linear equation in terms of the uknowns $ (a + b) $ and $ c $.\n",
    "\n",
    "$ y = (ax + b) / (cx + d) \\\\ $ \n",
    "No. This rational function cannot be estimated using linear regression. The denominator contains uknown coefficients c and d, making it non-linear in the parameters. Rational functions introduce non-linearity in the unknowns.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simple housing dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted prices for training data: [ -276129.03225809  1195483.87096774  -710322.58064517 -1143870.96774193\n",
      "   164516.12903224  -239677.41935483]\n",
      "Predicted prices for test data: [-510932.13795452  920267.67330146 -588398.67879184]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Training data\n",
    "X_train = np.array([\n",
    "    [1, 1500, 1, 0, 0],\n",
    "    [1, 2000, 0, 1, 0],\n",
    "    [1, 1800, 0, 0, 1],\n",
    "    [1, 2200, 1, 0, 0],\n",
    "    [1, 1700, 0, 1, 0],\n",
    "    [1, 2000, 0, 0, 1]\n",
    "])\n",
    "\n",
    "y_train = np.array([350000, 400000, 300000, 450000, 370000, 320000])\n",
    "\n",
    "# Computing Hat matrix\n",
    "XT_X_inv = np.linalg.inv(X_train.T @ X_train)\n",
    "H = X_train @ XT_X_inv @ X_train.T\n",
    "\n",
    "# Predicting using the Hat matrix\n",
    "y_hat_train = H @ y_train\n",
    "print(\"Predicted prices for training data:\", y_hat_train)\n",
    "\n",
    "# Test data\n",
    "X_test = np.array([\n",
    "    [1, 1900, 1, 0, 0],\n",
    "    [1, 1600, 0, 1, 0],\n",
    "    [1, 2100, 0, 0, 1]\n",
    "])\n",
    "\n",
    "# Compute coefficients (Î²) for prediction\n",
    "beta = XT_X_inv @ X_train.T @ y_train\n",
    "\n",
    "# Prediction for test data\n",
    "y_hat_test = X_test @ beta\n",
    "print(\"Predicted prices for test data:\", y_hat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary features (is_urban, is_suburban, is_rural) are linearly dependent because they always sum to 1. If we drop one of the binary features, the remaining two features would have been sufficient. \n",
    "\n",
    "The Size (sq. ft) feature ranges from 1500 to 2200, while the binary features take values of 0 or 1. So this large difference in scale causes numarical instability when inverting $ X^TX  $. I would standardize the Size column to have a mean of 0 and a standard deviation of 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I observe unrealistic predictions after calculating the Hat matrix. The predictions include negative numbers and very large values that are not consistent with the actual test values. This poor performance suggests that the Hat matrix is sensitive to the dataset's issues like multicollinearity and poor scaling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted prices for test data using scikit-learn: [406693.5483871  351532.25806452 336774.19354839]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# Train a regression model using scikit-learn\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train[:, 1:], y_train)  # Drop the intercept column for sklearn\n",
    "\n",
    "# Predict for test data\n",
    "y_pred_test = reg.predict(X_test[:, 1:])  # Drop the intercept column for sklearn\n",
    "print(\"Predicted prices for test data using scikit-learn:\", y_pred_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hat matrix prediction: [-510932.13795452  920267.67330146 -588398.67879184] $\\\\$\n",
    "Scikit-learn prediction: [406693.5483871  351532.25806452 336774.19354839] $\\\\$\n",
    "The asnwers from the Hat matrix approach do not match the Scikit-learn model's asnwer. \n",
    "The binary features (is_urban, is_suburban, is_rural) are linearly dependent and makes $ X^TX $ nearly singular. This results in numerical instability when calculating $ (X^TX)^{-1} \\\\ $\n",
    "The $ Size $ feature has a larger magnitude compared to the binary features and thus leads to skewed predictions. \n",
    "Scikit-learn, however, uses numerically stable methods to compute regression coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9298442842277389\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "# Reshape the true values to match the shape of y_pred_test\n",
    "true_values = np.array([420000, 360000, 330000])\n",
    "# Evaluating scikit-learn's regression model\n",
    "r2 = r2_score(true_values, y_pred_test)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Delivery time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "train_data = pd.read_csv('deliverytimeprediction_train.csv')\n",
    "test_data = pd.read_csv('deliverytimeprediction_test.csv')\n",
    "X_train = train_data.iloc[:, :-1].values\n",
    "y_train = train_data.iloc[:, -1].values\n",
    "\n",
    "X_test = test_data.iloc[:, :-1].values\n",
    "y_test = test_data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leverage Scores (Training): [0.01625162 0.02025266 0.01627211 0.0156765  0.03057456 0.01667397\n",
      " 0.03106329 0.01625457 0.01939773 0.01846582 0.0345463  0.01491957\n",
      " 0.01779681 0.02909732 0.01956296 0.01722316 0.01631803 0.03069324\n",
      " 0.02478875 0.03704939 0.015353   0.02206573 0.02316128 0.01487044\n",
      " 0.03612268 0.01631149 0.02930687 0.0263346  0.02975861 0.02713401\n",
      " 0.02714862 0.02706787 0.01311668 0.01802512 0.02400396 0.01930854\n",
      " 0.03902571 0.03034933 0.02471594 0.02783117 0.02742393 0.01635953\n",
      " 0.01872335 0.0141639  0.02183368 0.01470266 0.02639018 0.03462226\n",
      " 0.01119651 0.01499281 0.01434923 0.02498953 0.0293932  0.02668927\n",
      " 0.03235664 0.0202874  0.01474856 0.02927817 0.01883451 0.01170843\n",
      " 0.03144195 0.01868887 0.01653464 0.0252969  0.02740913 0.02167809\n",
      " 0.03077114 0.02279551 0.02726916 0.0179261  0.01813548 0.01818582\n",
      " 0.01394238 0.0192667  0.02769353 0.02978224 0.01287972 0.01936273\n",
      " 0.02542153 0.01292397 0.01813046 0.01739114 0.01214344 0.01516211\n",
      " 0.03281665 0.01550417 0.02869593 0.0183517  0.02586068 0.01387035\n",
      " 0.01285173 0.01094648 0.03299355 0.02745084 0.01713571 0.02474479\n",
      " 0.0194278  0.01553951 0.01211733 0.02201854 0.02539112 0.01372408\n",
      " 0.02159962 0.01667738 0.02613965 0.01541596 0.01839213 0.03269633\n",
      " 0.03638225 0.01581472 0.01822411 0.01371268 0.03068978 0.03476797\n",
      " 0.01975463 0.02027134 0.01531586 0.01949876 0.01085812 0.01811691\n",
      " 0.0299988  0.01772641 0.02455899 0.01798641 0.01188839 0.02325939\n",
      " 0.02130879 0.0168742  0.02111362 0.0164558  0.02726462 0.0171471\n",
      " 0.01740232 0.02788121 0.03632206 0.03237602 0.02274187 0.02516775\n",
      " 0.02468189 0.01368666 0.02226885 0.01078908 0.02022343 0.03342965\n",
      " 0.02858457 0.01674289 0.04074742 0.01884122 0.02595114 0.01930374\n",
      " 0.01106971 0.02883261 0.01913312 0.02764662 0.02148889 0.02279745\n",
      " 0.01508329 0.01458818 0.02443611 0.01601114 0.03028449 0.02065681\n",
      " 0.01953289 0.01759341 0.02455213 0.02710997 0.02813279 0.02403974\n",
      " 0.0128433  0.01334633 0.02920745 0.01874833 0.02357149 0.02091192\n",
      " 0.01626412 0.01275704 0.02051458 0.01100117 0.02365112 0.0254874\n",
      " 0.02857578 0.0227361  0.02640656 0.02536393 0.01072595 0.01389633\n",
      " 0.02773492 0.01238507 0.01872377 0.01571506 0.01484448 0.0201134\n",
      " 0.02250071 0.02120655 0.01475935 0.01786299 0.02047516 0.01138759\n",
      " 0.01430131 0.03912556 0.01362263 0.04145457 0.01270878 0.02241391\n",
      " 0.02462234 0.01652513 0.02825754 0.01996676 0.02575327 0.03012508\n",
      " 0.01164632 0.0252478  0.01599539 0.01887175 0.02890094 0.02296018\n",
      " 0.02149006 0.02386625 0.01982127 0.01995058 0.02710484 0.014709\n",
      " 0.02673037 0.01782862 0.01221654 0.01146082 0.02356044 0.03048519\n",
      " 0.01190511 0.02287015 0.02985443 0.02998698 0.01652969 0.03328383\n",
      " 0.03179314 0.01899803 0.02933798 0.03308438 0.03245225 0.02195944\n",
      " 0.02494701 0.03112075 0.01963526 0.01430592 0.01823437 0.02286205\n",
      " 0.01716109 0.02833384 0.03723737 0.04388211 0.01906481 0.03673266\n",
      " 0.02099105 0.01673274 0.01697412 0.01265474 0.01936092 0.01246606\n",
      " 0.02096476 0.03180938 0.02481998 0.02309399 0.01377717 0.0220571\n",
      " 0.01223705 0.01565854 0.01909718 0.02721056 0.02914847 0.01715694\n",
      " 0.02732483 0.02665422 0.03304756 0.02607794 0.01337313 0.01003312\n",
      " 0.02745286 0.01529408 0.02503561 0.01979829 0.02878507 0.01627015\n",
      " 0.03036608 0.01573122 0.0165082  0.02721412 0.01318504 0.02299579\n",
      " 0.01114029 0.0110398  0.02853616 0.01435607 0.02769295 0.01898898\n",
      " 0.01980523 0.02294599 0.01167519 0.01616925 0.02762687 0.01134685\n",
      " 0.02820404 0.04305928 0.03371231 0.01355927 0.0220955  0.01052449\n",
      " 0.02035843 0.01402804 0.01247957 0.02531551 0.02440393 0.01405719\n",
      " 0.0145777  0.01638641 0.02108969 0.02338984 0.0232125  0.02525545\n",
      " 0.03016129 0.02177215 0.02789832 0.0283144  0.0153213  0.02208219\n",
      " 0.0178556  0.01978662 0.01865788 0.01806971 0.02219067 0.01781627\n",
      " 0.02825928 0.02669522 0.01131155 0.01238775 0.02886578 0.01648509\n",
      " 0.01527993 0.02063885 0.02594333 0.01970121 0.01665968 0.01748597\n",
      " 0.01989135 0.02808175 0.0183781  0.01947456 0.02600672 0.0162017\n",
      " 0.01939779 0.01926437 0.01804331 0.01746905 0.01564406 0.01941311\n",
      " 0.01844005 0.01663646 0.02200972 0.02195129 0.02061041 0.02036291\n",
      " 0.02043824 0.01641066 0.03235544 0.01313658 0.01532742 0.01581312\n",
      " 0.01825124 0.01659832 0.01101469 0.02157526 0.01626408 0.02639705\n",
      " 0.02703362 0.02861032 0.01488806 0.0131849  0.02464655 0.01634988\n",
      " 0.01300629 0.03399422 0.01582504 0.02043901 0.01881453 0.02988371\n",
      " 0.01507768 0.0170952  0.01386807 0.01880596 0.0190564  0.01186364\n",
      " 0.01446138 0.014839   0.0281191  0.02128022 0.01953535 0.02293324\n",
      " 0.02489839 0.02381419 0.02327861 0.01979739 0.01157006 0.0186064\n",
      " 0.01869004 0.01516902 0.01933155 0.01810413 0.0138786  0.03395233\n",
      " 0.03755009 0.02700005 0.01490835 0.02131451 0.01881346 0.0186929\n",
      " 0.02040056 0.02914498 0.02535816 0.02978812 0.01172911 0.01502711\n",
      " 0.01768842 0.02810514 0.01645682 0.02220725 0.02399397 0.01603216\n",
      " 0.02461387 0.01604826 0.01415409 0.02590478 0.03617644 0.03105725\n",
      " 0.02364545 0.01750329 0.01916903 0.01400906 0.02504755 0.01538921\n",
      " 0.02493898 0.01668334 0.01630324 0.02263932 0.01732686 0.02126485\n",
      " 0.01179654 0.03349191 0.01890856 0.01984185 0.01470991 0.01282676\n",
      " 0.02662482 0.02176278 0.01818505 0.01913858 0.01611206 0.02341157\n",
      " 0.01334217 0.03003571 0.02708958 0.02446539 0.02594127 0.03681429\n",
      " 0.0168691  0.02619824 0.01585264 0.03482236 0.02008452 0.01347132\n",
      " 0.02191475 0.01070661 0.02027806 0.01659866 0.02600858 0.01999509\n",
      " 0.02476095 0.01760385 0.01623895 0.03056473 0.0200971  0.01325934\n",
      " 0.02420321 0.03314807 0.01112502 0.02355398 0.0260243  0.01539818\n",
      " 0.01549847 0.02072344 0.03670216 0.01829773 0.02990798 0.02357375\n",
      " 0.02188729 0.02608632 0.01822651 0.02235535 0.01718558 0.01074062\n",
      " 0.02089059 0.01500306 0.01145872 0.03319493 0.01636297 0.01713735\n",
      " 0.01748168 0.01406661 0.01303878 0.02866811 0.02875655 0.02721789\n",
      " 0.01734252 0.02270691 0.03078676 0.01365658 0.01858036 0.02088446\n",
      " 0.03340444 0.01665946 0.02618217 0.01260321 0.02765868 0.01968095\n",
      " 0.01749214 0.0133882  0.01344531 0.02955772 0.02116995 0.0274278\n",
      " 0.01409374 0.03232994 0.01710572 0.02688588 0.01537264 0.03031508\n",
      " 0.01371943 0.01792783 0.02219228 0.01476679 0.01473539 0.01972737\n",
      " 0.02585974 0.01337907 0.03008607 0.02502467 0.03056957 0.02840898\n",
      " 0.03030357 0.01348299 0.01794796 0.01621991 0.02181523 0.01962105\n",
      " 0.01721313 0.01340204 0.01851153 0.01915555 0.01494413 0.02172691\n",
      " 0.02654789 0.01880732 0.01992846 0.02887297 0.02521555 0.03370195\n",
      " 0.02197794 0.0221463  0.02256503 0.02921066 0.02964381 0.02275882\n",
      " 0.02686531 0.02925325 0.01904356 0.02550355 0.02470905 0.03241605\n",
      " 0.03757532 0.01458463 0.01985158 0.01627136 0.03041872 0.0144695\n",
      " 0.01749948 0.01925568 0.02079498 0.02471862 0.01291841 0.01990483\n",
      " 0.01976438 0.01765938 0.03470491 0.01278384 0.01896571 0.01840616\n",
      " 0.02440823 0.01686535 0.01364378 0.02917198 0.01458057 0.02755555\n",
      " 0.01917904 0.02883363 0.02293155 0.01321218 0.02712593 0.01574313\n",
      " 0.02012991 0.03678255 0.02477296 0.01530639 0.02006102 0.01923261\n",
      " 0.02925558 0.01279083 0.01656414 0.02831536 0.03696632 0.02754306\n",
      " 0.01623978 0.01036413 0.0248561  0.01765076 0.02417631 0.01388737\n",
      " 0.02153535 0.02024178 0.02140608 0.02401083 0.01811161 0.02532584\n",
      " 0.02169242 0.03909298 0.02138436 0.02403171 0.02459046 0.02738488\n",
      " 0.02334017 0.02089077 0.02474277 0.01156017 0.01977692 0.01674164\n",
      " 0.01812431 0.01606332 0.01352671 0.01953331 0.01838252 0.02537184\n",
      " 0.02954946 0.01602294 0.01352765 0.02856646 0.02032815 0.02533278\n",
      " 0.02140689 0.01188661 0.01510209 0.01182758 0.02279616 0.02367865\n",
      " 0.02225117 0.01754416 0.02331521 0.02280428 0.03346937 0.02713079\n",
      " 0.01722682 0.01406695 0.01864874 0.01398088 0.02849675 0.02814572\n",
      " 0.03932301 0.01703478 0.02961613 0.0133554  0.02707592 0.01334338\n",
      " 0.03144937 0.02734268 0.01388065 0.01811759 0.02269487 0.0190671\n",
      " 0.02776763 0.01339274 0.02660756 0.03499803 0.01349805 0.01422484\n",
      " 0.02452329 0.02917746 0.0229768  0.03256951 0.01997128 0.01142277\n",
      " 0.01346602 0.01202691 0.02678221 0.02791242]\n"
     ]
    }
   ],
   "source": [
    "# Convert boolean values to integers\n",
    "X_train = X_train.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "\n",
    "# Add an intercept term\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Fit OLS Regression Model\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "# Compute Hat Matrix (for training data)\n",
    "XTX_inv = np.linalg.inv(X_train.T @ X_train)\n",
    "H_train = X_train @ XTX_inv @ X_train.T\n",
    "\n",
    "# Extract leverage values (diagonal elements of H)\n",
    "leverage_train = np.diag(H_train)\n",
    "\n",
    "# Compute residuals for training data\n",
    "y_pred_train = model.predict(X_train)\n",
    "residuals_train = y_train - y_pred_train\n",
    "\n",
    "# Identify high-leverage points\n",
    "high_leverage_threshold = 2 * (X_train.shape[1]) / X_train.shape[0]  # 2 * (p / n)\n",
    "high_leverage_points = np.where(leverage_train > high_leverage_threshold)[0]\n",
    "\n",
    "# Identify large residuals in training set\n",
    "large_residuals_train = np.where(np.abs(residuals_train) > 2 * residuals_train.std())[0]\n",
    "\n",
    "print(\"Leverage Scores (Training):\", leverage_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-Leverage Points (Training): [249 301]\n"
     ]
    }
   ],
   "source": [
    "print(\"High-Leverage Points (Training):\", high_leverage_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Residuals (Training): [ 13 115 156 165 198 222 247 359 366 382 391 418 420 484 485 509 534 574\n",
      " 599 644 645 691 693]\n"
     ]
    }
   ],
   "source": [
    "print(\"Large Residuals (Training):\", large_residuals_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Influence and residuals together provide us with insights into which data points have the most impact on the regression model and whether certain points are outliers or ditort the model. $ \\\\ $\n",
    "High leverage or influence points are far from the mean of the predictor variables. These points can significantly change the regression line. $ \\\\ $\n",
    "Residual or a prediction error for a point show the difference between actual values and predicted ones. Large residuals mean the model made a significant prediction error for that data point. $ \\\\ $\n",
    "Some predictions here are way off, most likely because the model isn't capturing key patterns, like unusual traffic conditions or extreme preparation times. If these points also have high leverage, they might be distorting the model. $ \\\\ $\n",
    "To fix this, we could identify outliers, use a more robust regression model, or revisit feature selection to improve accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementing regression approaches and compare and contrast results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load dataset\n",
    "train_data = pd.read_csv('deliverytimeprediction_train.csv')\n",
    "test_data = pd.read_csv('deliverytimeprediction_test.csv')\n",
    "\n",
    "# Split features and target\n",
    "X_train = train_data.iloc[:, :-1]\n",
    "y_train = train_data.iloc[:, -1]\n",
    "X_test = test_data.iloc[:, :-1]\n",
    "y_test = test_data.iloc[:, -1]\n",
    "\n",
    "# Convert boolean values to floats for numerical computation\n",
    "X_train = X_train.astype(float)\n",
    "X_test = X_test.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression by Successive Orthogonalization\n",
    "\n",
    "sequentially removes correlation between predictors and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successive Orthogonalization Coefficients:\n",
      "Feature: Distance_km, Coefficient: 5.0363\n",
      "Feature: Preparation_Time_min, Coefficient: 1.2933\n",
      "Feature: Courier_Experience_yrs, Coefficient: -0.0409\n",
      "Feature: Weather_Foggy, Coefficient: 6.2924\n",
      "Feature: Weather_Rainy, Coefficient: 4.7852\n",
      "Feature: Weather_Snowy, Coefficient: 9.6200\n",
      "Feature: Weather_Windy, Coefficient: 5.0494\n",
      "Feature: Traffic_Level_Low, Coefficient: -6.2648\n",
      "Feature: Traffic_Level_Medium, Coefficient: -1.5293\n",
      "Feature: Time_of_Day_Evening, Coefficient: 2.6785\n",
      "Feature: Time_of_Day_Morning, Coefficient: 3.1431\n",
      "Feature: Time_of_Day_Night, Coefficient: 2.9864\n",
      "Feature: Vehicle_Type_Car, Coefficient: 1.4514\n",
      "Feature: Vehicle_Type_Scooter, Coefficient: 1.0143\n",
      "\n",
      "Predicted Delivery Times for Test Data:\n",
      "[ 56.23014193  33.76506748  50.88935481  62.09751402  79.1634922\n",
      "  43.43177071  73.33844822 100.68730618  52.07913592  44.43577785\n",
      "  30.45447548  37.55312626  22.89027291  65.5756177   50.80955356\n",
      "  85.86093968  50.55274784  69.20711269  62.44647366  77.49226833\n",
      "  59.91561196  82.80736889  75.66561921  65.94857927  26.85532159\n",
      "  69.65529894  44.34573163  54.39441116  83.82329179  75.72800091\n",
      "  86.21575861  82.87697098  24.83129616  80.27231203  23.42369961\n",
      "  69.58572687  39.70643565  50.55366397  90.55894134  32.57112012\n",
      "  60.69877202  36.3286977   14.05064888  23.6529801   52.95620366\n",
      "  98.6673565   72.31968066  47.89220103  48.84473424  83.41007538\n",
      "  12.50525973  50.83702644  69.97149977  52.3589268   68.17411281\n",
      "  73.05248943  24.34472073  38.26424648  38.04827266  93.24182511\n",
      "  75.91040036  19.76180991  36.35593384  77.32709139  58.663045\n",
      "  49.17994228  62.67634836  46.30899745  63.06044434  98.02238302\n",
      "  28.00585111  56.61791116  93.76076426  11.2561915   54.41255876\n",
      "  68.9826506   31.60096196  70.6230643   65.53347187  64.984083\n",
      "  39.21345997  95.19695123  83.44429625  62.60447136  40.75557322\n",
      "  78.95878937  54.57630014  27.65415472  12.25035316  61.05135873\n",
      "  69.26937444  23.0397617   59.62862222  32.3229211   52.86918537\n",
      "  30.23967038  52.30918771  69.41892299  39.91459077  68.25552377\n",
      "  87.61318954  15.74494151  52.2233734   76.82702196  53.34039765\n",
      "  76.8633147   73.65296578  10.65188833  79.10709299  59.73949224\n",
      "  71.5983815   56.20803457  86.85855081  54.80952128  68.45341\n",
      "  47.68980588  51.03660522  33.43543885  33.59603988  33.92938991\n",
      "  38.43677888  54.70964401  72.33124151  42.94403851  41.81794729\n",
      "  46.5674642   61.95283277  72.77576152  18.41896621  64.31026417\n",
      "  80.01776943  65.61523235  47.54799511  69.7388749   66.21414313\n",
      "  49.88202683  29.00518078  94.89580669  24.75231782 101.09908503\n",
      "  90.28445457  38.99519944  70.56746241  50.89745718  65.62784582\n",
      "  88.05530599  46.06781502  99.05193455  78.27064086  57.22216867\n",
      "  50.22947952  28.49010415  83.5154542   34.25745734  46.9097757\n",
      "  59.34545663  94.84373687  23.28636379  48.48051429  57.8407941\n",
      "  73.3823392   58.96449131  41.64407306  65.6722504   30.90744944\n",
      "  42.76134396  48.3162114   44.30560451  51.06663745  72.33128474\n",
      "  57.860983    70.06890842  82.95993661  39.87266364  59.09949902\n",
      "  51.70281813  97.53635087  32.84623789  48.103236    77.84583019\n",
      "  31.61462883  37.70895965  44.69060322  33.49557718  87.44856714\n",
      "  49.09724861  43.44078678  67.61206561  23.08271008  29.35472163\n",
      "  45.23080633  46.48854246  73.05403949  28.10312877  68.78689748\n",
      "  23.81203649  35.33343632  40.88280377  55.11394846  33.98545354\n",
      "  54.93934435  22.85178851  56.05712321  42.91012169  72.10167172\n",
      "  36.97637156  62.45220613  87.91765793  69.51533405  79.14937649\n",
      "  49.34659275  52.52520788  48.66984644  82.06187343  61.5599883\n",
      "  39.28142135  43.29164442  57.45259768  94.05653336  76.52079669\n",
      "  38.55391938  64.06734622  76.6091649   48.98315088  14.68908939\n",
      "  27.55784326  45.78293096  77.28064219  80.56484668  28.29888121\n",
      "  38.56006797  33.22620823  39.57291324  60.82814593  39.38254197\n",
      "  34.4158514   82.34521819  60.35946698  31.63878358  84.48652206\n",
      "  64.79850927  95.73482564  76.84113494  59.47763703  55.29557683\n",
      "  87.63878818  52.8912577   33.88998339  41.78952641  24.99959722\n",
      "  58.54810361  33.8173953   87.16434634  20.54481755  63.27736282\n",
      "   7.08279275  33.29258696  35.00999463  71.60784527  37.62944061\n",
      "  88.78781618  62.41768361  21.50589507  77.55151071  32.4630313\n",
      "  80.86138546  59.75845754  11.06738703  50.79138812  50.8078316\n",
      "  43.96543888  22.04542784  43.31441232  60.97870979  71.01119721\n",
      "  87.0690479   21.34300547  48.83665184  29.7725111   81.06026272\n",
      "  26.79812266  34.96099715  71.42965062  61.86557701  10.1780174\n",
      "  54.72179279  44.93049286  47.11821617  55.20520152  58.63909448\n",
      "  79.62639416  33.14374312  65.67609357  31.25398662  39.09507672\n",
      "  59.32573073  79.18823002  52.50309001  24.26917326  59.26122914]\n",
      "\n",
      "Model Performance:\n",
      "Mean Squared Error: 93.4374\n",
      "R-Squared Score: 0.7955\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load dataset\n",
    "train_data = pd.read_csv('deliverytimeprediction_train.csv')\n",
    "test_data = pd.read_csv('deliverytimeprediction_test.csv')\n",
    "\n",
    "# Split features and target\n",
    "X_train = train_data.iloc[:, :-1]\n",
    "y_train = train_data.iloc[:, -1]\n",
    "X_test = test_data.iloc[:, :-1]\n",
    "y_test = test_data.iloc[:, -1]\n",
    "\n",
    "# Convert boolean values to floats for numerical computation\n",
    "X_train = X_train.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "\n",
    "def successive_orthogonalization(X_train, y_train, X_test, y_test):\n",
    "    \"\"\" Performs regression by Successive Orthogonalization. \"\"\"\n",
    "    n_samples_train, n_features = X_train.shape\n",
    "    n_samples_test = X_test.shape[0]\n",
    "\n",
    "    coefficients = {}  # Store regression coefficients\n",
    "    residuals = y_train.copy()  # Start with original target values\n",
    "\n",
    "    feature_names = X_train.columns  \n",
    "    X_train_np = X_train.values  \n",
    "    X_test_np = X_test.values  \n",
    "    y_test_np = y_test.values  \n",
    "\n",
    "    # Storage for orthogonalized features\n",
    "    Z_train = np.zeros_like(X_train_np)\n",
    "    Z_test = np.zeros_like(X_test_np)\n",
    "\n",
    "    for i in range(n_features):\n",
    "        feature_name = feature_names[i]\n",
    "        x_i_train = X_train_np[:, i].copy()\n",
    "        x_i_test = X_test_np[:, i].copy()  # Keep test feature for transformation\n",
    "\n",
    "        # Orthogonalization\n",
    "        for j in range(i):\n",
    "            x_j_train = Z_train[:, j]  # Use previously orthogonalized feature\n",
    "            x_j_test = Z_test[:, j]  # Apply same transformation on test data\n",
    "\n",
    "            # Compute projection coefficient (scalar)\n",
    "            proj_coeff = np.dot(x_i_train, x_j_train) / np.dot(x_j_train, x_j_train)\n",
    "\n",
    "            # Apply projection to both train and test\n",
    "            x_i_train -= proj_coeff * x_j_train\n",
    "            x_i_test -= proj_coeff * x_j_test  # Ensure correct transformation\n",
    "\n",
    "        # Store orthogonalized feature\n",
    "        Z_train[:, i] = x_i_train\n",
    "        Z_test[:, i] = x_i_test\n",
    "\n",
    "        # Compute regression coefficient\n",
    "        coefficient = np.dot(x_i_train, residuals) / np.dot(x_i_train, x_i_train)\n",
    "        residuals -= coefficient * x_i_train  # Update residuals\n",
    "        coefficients[feature_name] = coefficient\n",
    "\n",
    "    # Compute final predictions for test data\n",
    "    y_pred_test = np.dot(Z_test, np.array(list(coefficients.values())))\n",
    "\n",
    "    # Evaluate performance\n",
    "    mse = mean_squared_error(y_test_np, y_pred_test)\n",
    "    r2 = r2_score(y_test_np, y_pred_test)\n",
    "\n",
    "    return coefficients, y_pred_test, mse, r2\n",
    "\n",
    "# Run successive orthogonalization regression\n",
    "coefficients, y_pred_test, mse, r2 = successive_orthogonalization(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Print coefficients\n",
    "print(\"\\nSuccessive Orthogonalization Coefficients:\")\n",
    "for feature_name, coeff in coefficients.items():\n",
    "    print(f\"Feature: {feature_name}, Coefficient: {coeff:.4f}\")\n",
    "\n",
    "# Print final predictions\n",
    "print(\"\\nPredicted Delivery Times for Test Data:\")\n",
    "print(y_pred_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-Squared Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "uses L1 regularization, which eliminates less important features by shrinking some coefficients to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha (regularization parameter): 0.097073\n",
      "\n",
      "Lasso Regression Performance:\n",
      "RMSE: 9.0074\n",
      "RÂ² Score: 0.8225\n",
      "\n",
      "Eliminated Features (Regularized to Zero):\n",
      "['Time_of_Day_Morning', 'Time_of_Day_Night', 'Vehicle_Type_Car']\n",
      "\n",
      "Retained Features:\n",
      "['Distance_km', 'Preparation_Time_min', 'Courier_Experience_yrs', 'Weather_Foggy', 'Weather_Rainy', 'Weather_Snowy', 'Weather_Windy', 'Traffic_Level_Low', 'Traffic_Level_Medium', 'Time_of_Day_Evening', 'Vehicle_Type_Scooter']\n",
      "\n",
      "Predicted Delivery Times for Test Data:\n",
      "[55.23365443 34.17921636 46.79325866 67.44691746 77.48319161 52.83649132\n",
      " 72.95992379 98.34195197 54.20685845 44.25623012 31.26327303 40.90018576\n",
      " 25.15207744 69.27702157 53.15017728 88.53457797 50.75563797 69.37993888\n",
      " 62.74919236 76.0066539  53.32650487 84.05361997 76.03512781 65.63137428\n",
      " 33.16263644 64.76066574 39.22323097 62.7841507  80.36127835 72.8291333\n",
      " 84.46630082 86.02742739 20.78630688 80.6585118  25.78962425 68.49260353\n",
      " 36.37055623 59.71372699 85.50172183 37.87929768 56.54165799 35.94553313\n",
      " 20.88519295 30.69615126 53.47894488 97.08660469 72.07074355 53.54765758\n",
      " 49.56198692 78.84456074 19.31050307 48.17047611 70.78993657 58.96107408\n",
      " 66.33603362 79.00808263 31.07652208 41.65940954 37.43762554 88.57087132\n",
      " 79.59927071 29.47561858 42.15879574 77.02441609 58.87266831 44.15841129\n",
      " 61.45980638 49.32956101 64.1872007  87.44428659 31.60381521 58.75854872\n",
      " 85.29266136 20.90441846 58.97894496 67.9400344  34.17296981 68.96544959\n",
      " 69.47528014 65.4772867  41.45494121 92.71547857 79.82237179 64.20682096\n",
      " 40.30402045 76.29431593 54.83341864 30.1162003  15.875866   57.60197091\n",
      " 71.03588897 25.96609975 60.83127891 33.61995987 56.77768906 35.82051048\n",
      " 52.64873981 69.97515618 38.48043259 66.59660302 87.34197127 16.0200827\n",
      " 52.1715924  74.18420722 53.8481484  75.18945719 77.65206947 19.90915586\n",
      " 78.75153749 63.61535483 68.76574694 52.45058372 86.37068653 56.88982534\n",
      " 68.42520495 57.0407229  53.4391061  33.95429022 39.19732271 36.43915102\n",
      " 45.86502289 52.95234315 72.64604128 44.4069116  45.84763915 46.57929314\n",
      " 60.85344427 71.07092022 25.28337069 60.25492721 78.46224238 64.10848865\n",
      " 52.19483021 68.40664208 64.23665107 53.45103748 38.48577495 90.34311686\n",
      " 33.65960084 99.33063148 85.35781493 43.4069062  66.27888354 52.9671441\n",
      " 72.31443266 81.80954923 42.68664974 96.54993283 74.82066421 55.04363679\n",
      " 48.85712284 34.48450679 79.16829105 36.172137   47.89571259 60.36432643\n",
      " 91.55232693 30.59151781 50.29539867 56.05483657 71.7205852  62.3284172\n",
      " 42.708796   65.15711982 31.66771934 46.53616282 49.86081762 46.47386709\n",
      " 50.79153138 76.61200871 60.33808356 67.38378058 82.34631499 45.97935488\n",
      " 62.84509741 55.11979155 92.11766496 37.81479575 50.43732728 78.85764541\n",
      " 34.0182207  42.89532662 50.72678421 35.22640719 84.88351974 47.80501203\n",
      " 42.43900816 64.93572067 30.04503442 36.69710667 44.79690351 43.30644023\n",
      " 73.09992075 34.65598888 66.40909538 29.13779242 31.90745909 46.30273395\n",
      " 50.71760299 42.40373759 54.85546875 28.52340188 54.99814958 42.83017827\n",
      " 73.34440478 43.86932371 65.89084969 82.90931116 69.8901182  73.9566622\n",
      " 49.72003728 44.90012355 55.98432214 80.32970208 55.87946937 44.91784849\n",
      " 50.81566627 57.83921412 94.99309762 70.10649315 36.32690642 61.41781962\n",
      " 74.04571646 55.75379706 22.05930663 30.59738247 55.2583424  76.15047878\n",
      " 75.95735211 32.9077675  46.49483877 40.70822559 41.69093741 58.39486391\n",
      " 38.89832945 35.31046403 77.63410719 62.79334981 33.89772874 80.98888456\n",
      " 64.49673517 94.65121687 77.8088316  60.4882828  57.21218507 81.74607608\n",
      " 65.03482504 36.92812835 45.95056744 30.39126989 52.61293669 37.59988573\n",
      " 88.98742232 23.0717256  62.71003639 17.65271948 35.89760149 37.1909318\n",
      " 67.95050078 39.61416737 92.43041687 63.44215351 26.7528064  77.26700455\n",
      " 35.55377112 80.49714901 63.66957729 21.84379815 53.39858158 56.85295813\n",
      " 39.7589754  26.01269062 42.6766655  64.99175252 71.42435363 84.45835138\n",
      " 27.8460862  45.42684018 33.18084011 78.42069695 27.55150211 40.06845828\n",
      " 76.35695454 62.64316373 16.35927141 50.37112198 46.22297026 51.6735005\n",
      " 58.42819085 62.03674434 80.01479522 35.61950013 61.44133828 35.12634967\n",
      " 38.07569037 66.17492345 81.21089805 60.71340886 24.33313475 61.26938388]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV, Ridge, LassoCV, Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define the LassoCV model with cross-validation\n",
    "lasso_cv = LassoCV(cv=KFold(n_splits=5, shuffle=True, random_state=42), random_state=42)  # 5-fold cross-validation\n",
    "\n",
    "# Fit the model to the training data\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "# Get the best alpha (regularization parameter)\n",
    "best_alpha = lasso_cv.alpha_\n",
    "print(f\"Best alpha (regularization parameter): {best_alpha:.6f}\")\n",
    "\n",
    "# Fit the final Lasso model with the best alpha\n",
    "lasso_best = Lasso(alpha=best_alpha, random_state=42)\n",
    "lasso_best.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_lasso = lasso_best.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "print(f\"\\nLasso Regression Performance:\")\n",
    "print(f\"RMSE: {rmse_lasso:.4f}\")\n",
    "print(f\"RÂ² Score: {r2_lasso:.4f}\")\n",
    "\n",
    "# Identify which features are eliminated (coefficients close to zero)\n",
    "feature_names = X_train.columns\n",
    "coefficients = lasso_best.coef_\n",
    "\n",
    "eliminated_features = [feature for feature, coef in zip(feature_names, coefficients) if abs(coef) < 1e-4]\n",
    "retained_features = [feature for feature, coef in zip(feature_names, coefficients) if abs(coef) >= 1e-4]\n",
    "\n",
    "print(\"\\nEliminated Features (Regularized to Zero):\")\n",
    "print(eliminated_features)\n",
    "\n",
    "print(\"\\nRetained Features:\")\n",
    "print(retained_features)\n",
    "\n",
    "# Print final predictions\n",
    "print(\"\\nPredicted Delivery Times for Test Data:\")\n",
    "print(y_pred_lasso)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "applies L2 regularization, which shrinks coefficients without eliminating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha (regularization parameter): 1.000000\n",
      "\n",
      "Ridge Regression Performance:\n",
      "RMSE: 9.0474\n",
      "RÂ² Score: 0.8209\n",
      "\n",
      "Ridge Regression Coefficients:\n",
      "Distance_km: 2.9782\n",
      "Preparation_Time_min: 0.9584\n",
      "Courier_Experience_yrs: -0.6959\n",
      "Weather_Foggy: 7.3734\n",
      "Weather_Rainy: 4.9829\n",
      "Weather_Snowy: 9.7019\n",
      "Weather_Windy: 1.8006\n",
      "Traffic_Level_Low: -12.3500\n",
      "Traffic_Level_Medium: -6.8151\n",
      "Time_of_Day_Evening: 1.4281\n",
      "Time_of_Day_Morning: 0.3685\n",
      "Time_of_Day_Night: -0.2891\n",
      "Vehicle_Type_Car: 0.2946\n",
      "Vehicle_Type_Scooter: -1.1179\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the RidgeCV model with cross-validation\n",
    "ridge_cv = RidgeCV(alphas=np.logspace(-6, 6, 13), cv=KFold(n_splits=5, shuffle=True, random_state=42))\n",
    "\n",
    "# Fit the model to the training data\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "# Get the best alpha (regularization parameter)\n",
    "best_alpha = ridge_cv.alpha_\n",
    "print(f\"Best alpha (regularization parameter): {best_alpha:.6f}\")\n",
    "\n",
    "# Fit the final Ridge model with the best alpha\n",
    "ridge_best = Ridge(alpha=best_alpha, random_state=42)\n",
    "ridge_best.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_ridge = ridge_best.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "print(f\"\\nRidge Regression Performance:\")\n",
    "print(f\"RMSE: {rmse_ridge:.4f}\")\n",
    "print(f\"RÂ² Score: {r2_ridge:.4f}\")\n",
    "\n",
    "# Compare coefficients of Ridge with Lasso and Successive Orthogonalization\n",
    "ridge_coefficients = ridge_best.coef_\n",
    "\n",
    "# Print coefficients in a structured format\n",
    "print(\"\\nRidge Regression Coefficients:\")\n",
    "for feature, coef in zip(X_train.columns, ridge_coefficients):\n",
    "    print(f\"{feature}: {coef:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successive Orthogonalization - MSE: 399.6590925022123, R2: 0.12546247305375047, RSS: 119897.72775066367\n",
      "Lasso Regression - MSE: 81.13287934001247, R2: 0.8224643227611537, RSS: 24339.86380200374\n",
      "Ridge Regression - MSE: 81.85553782400548, R2: 0.8208829951364995, RSS: 24556.661347201643\n"
     ]
    }
   ],
   "source": [
    "# Compute MSE, R^2, and RSS for all models\n",
    "\n",
    "# Successive Orthogonalization\n",
    "y_pred_successive = np.dot(X_test, ridge_coefficients)\n",
    "mse_successive = mean_squared_error(y_test, y_pred_successive)\n",
    "r2_successive = r2_score(y_test, y_pred_successive)\n",
    "rss_successive = np.sum((y_test - y_pred_successive) ** 2)\n",
    "\n",
    "print(f\"Successive Orthogonalization - MSE: {mse_successive}, R2: {r2_successive}, RSS: {rss_successive}\")\n",
    "\n",
    "# Lasso Regression\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "rss_lasso = np.sum((y_test - y_pred_lasso) ** 2)\n",
    "\n",
    "print(f\"Lasso Regression - MSE: {mse_lasso}, R2: {r2_lasso}, RSS: {rss_lasso}\")\n",
    "\n",
    "# Ridge Regression\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "rss_ridge = np.sum((y_test - y_pred_ridge) ** 2)\n",
    "\n",
    "print(f\"Ridge Regression - MSE: {mse_ridge}, R2: {r2_ridge}, RSS: {rss_ridge}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
