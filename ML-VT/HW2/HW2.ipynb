{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient Descent\n",
    "\n",
    "Let's create the dataset frist. We are given $x$-values: $ -1, 0, 1, 2 $. Let's compute the corresponsinf $y$-values using the true function: $y = 3x^3 + 4x^2 + 5x + 6$\n",
    "\n",
    "For $x = -1 \\\\$ \n",
    "$y = 3(-1)^3 + 4(-1)^2 + 5(-1) + 6 = -3 + 4 - 5 + 6 = 2 \\\\$\n",
    "\n",
    "For $x = 0 \\\\$\n",
    "$y = 3(0)^3 + 4(0)^2 + 5(0) + 6 = 6 \\\\$\n",
    "\n",
    "For $x = 1 \\\\$\n",
    "$y = 3(1)^3 + 4(1)^2 + 5(1) + 6 = 3 + 4 + 5 + 6 = 18 \\\\$\n",
    "\n",
    "For $x = 2 \\\\$\n",
    "$y = 3(2)^3 + 4(2)^2 + 5(2) + 6 = 24 + 16 + 10 + 6 = 56 \\\\$\n",
    "\n",
    "Thus, our dataset is: ${(-1, 2), (0, 6), (1, 18), (2, 56)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to minimize the Mean Squared Error (MSE):\n",
    "\n",
    "$J(a, b, c, d) = \\frac{1}{4} \\displaystyle\\sum_{i=1}^{4} ((ax_i^3 + bx_i^2 + cx_i + d) - y_i)^2  \\\\$\n",
    "\n",
    "where: $a, b, c, d$ are our parameters to be optimized. $x_i$  and  $y_i$ are the values from our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now computer partial deriveratives for Gradient Descent. Gradient descent updates parameters as: $\\\\$\n",
    "\n",
    "$a = a - \\alpha \\frac{\\partial J}{\\partial a} \\\\$\n",
    "\n",
    "$b = b - \\alpha \\frac{\\partial J}{\\partial b} \\\\$\n",
    "\n",
    "$c = c - \\alpha \\frac{\\partial J}{\\partial c} \\\\$\n",
    "\n",
    "$d = d - \\alpha \\frac{\\partial J}{\\partial d} \\\\$\n",
    "\n",
    "where $\\alpha  = 0.0.1 $ is the learning rate.\n",
    "\n",
    "$\\frac{\\partial J}{\\partial a} = \\frac{1}{4} \\displaystyle\\sum_{i=1}^{4} 2(ax_i^3 + bx_i^2 + c x_i + d - y_i)x_i^3$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial b} = \\frac{1}{4} \\displaystyle\\sum_{i=1}^{4} 2(ax_i^3 + bx_i^2 + c x_i + d - y_i)x_i^2\n",
    "$\n",
    "\n",
    "\n",
    "$\\frac{\\partial J}{\\partial c} = \\frac{1}{4} \\displaystyle\\sum_{i=1}^{4} 2(ax_i^3 + bx_i^2 + c x_i + d - y_i)x_i$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial d} = \\frac{1}{4} \\displaystyle\\sum_{i=1}^{4} 2(ax_i^3 + bx_i^2 + c x_i + d - y_i)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue calculating using the following Python snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: a=2.7000, b=1.9000, c=1.4700, d=1.3100\n",
      "Iteration 2: a=3.6403, b=2.4189, c=1.7338, d=1.5141\n",
      "Iteration 3: a=4.1554, b=2.7240, c=1.8823, d=1.6583\n",
      "\n",
      "Final Predictions: [-1.655356  1.658301 10.41991  49.561697]\n",
      "True Values: [ 2  6 18 56]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the true function\n",
    "def true_function(x):\n",
    "    return 3*x**3 + 4*x**2 + 5*x + 6\n",
    "\n",
    "# Create dataset\n",
    "x_train = np.array([-1, 0, 1, 2])\n",
    "y_train = true_function(x_train)\n",
    "\n",
    "# Initialize parameters\n",
    "a, b, c, d = 1.0, 1.0, 1.0, 1.0  # Initial values\n",
    "learning_rate = 0.01\n",
    "num_iterations = 3  # Perform 3 iterations\n",
    "\n",
    "# Gradient Descent Loop\n",
    "for iteration in range(1, num_iterations + 1):\n",
    "    # Compute predictions\n",
    "    y_pred = a * x_train**3 + b * x_train**2 + c * x_train + d\n",
    "\n",
    "    # Compute errors\n",
    "    error = y_pred - y_train\n",
    "\n",
    "    # Compute gradients\n",
    "    grad_a = np.mean(2 * error * x_train**3)\n",
    "    grad_b = np.mean(2 * error * x_train**2)\n",
    "    grad_c = np.mean(2 * error * x_train)\n",
    "    grad_d = np.mean(2 * error)\n",
    "\n",
    "    # Update parameters\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Iteration {iteration}: a={a:.4f}, b={b:.4f}, c={c:.4f}, d={d:.4f}\")\n",
    "\n",
    "# Final predicted values\n",
    "y_final_pred = a * x_train**3 + b * x_train**2 + c * x_train + d\n",
    "print(\"\\nFinal Predictions:\", y_final_pred)\n",
    "print(\"True Values:\", y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In iteration 1, the parameters changed from $ a = 1, b = 1, c = 1, d = 1 $ to $a = 2.7000, b = 1.9000, c = 1.4700, d = 1.3100. \\\\$\n",
    "The coefficients moved significantly toward correct values ($a=3, b=4, c=5, d=6$). The learning process is making a noticeable impact on $a$, which has the largest jump $(+1.7)$, meaning the cubic term had the most influence in reducing error. $\\\\$\n",
    "\n",
    "In iteration 2, parameters were updated to $a=3.6403, b=2.4189, c=1.7338, d=1.5141 \\\\$\n",
    "The coefficients keep moving in the right direction. $a$ is getting closer to 3, while $b, c, d$ are still far from their correct values. The model is improving, but it is clear that more iterations are needed for full convergence. $\\\\$\n",
    "\n",
    "In iteration 3, parameters are updated to $a=4.1554, b=2.7240, c=1.8823, d=1.6583$. $a$ overshoots 3, suggesting that the gradient descent updates might be slightly aggressive. $b, c, d$ are still improving but have not reached their true values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| $x$  | Final Prediction ($\\hat{y}$) | True Value ($y$) | Error  |\n",
    "|------|----------------|------------|--------|\n",
    "| $-1$ | $-1.6554$      | $2$        | $-3.66$ |\n",
    "| $0$  | $1.6583$       | $6$        | $-4.34$ |\n",
    "| $1$  | $10.4199$      | $18$       | $-7.58$ |\n",
    "| $2$  | $49.5617$      | $56$       | $-6.44$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are improving but are still far off from the true values. Negative values for $ùë• = -1$ indicate that the model is struggling in certain regions. The gap between predictions and actual values shows that the model has not yet converged. I think more iterations would likely reduce the error further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">Will the Algorithm Converge? </span> Yes. The updates in each step are moving the parameters toward the correct values, but they are still far off after three iterations.\n",
    "If we increase the number of iterations, the error should continue decreasing, leading to better estimates. The learning rate in our case is 0.01. If it is too small, convergence will be very slow. If it is too large, it might cause the updates to overshoot, leading to oscillations or divergence. \n",
    "\n",
    "We should also note that the dataset contains only four points $x=[‚àí1,0,1,2]$, which is very limited for learning a cubic function.\n",
    "If we had more data points covering a wider range of $x$, the gradient estimates would be more stable, helping the algorithm converge more reliably.\n",
    "\n",
    "Gradient descent is guaranteed to converge only if the loss function is convex. In our case, we are fitting a cubic function, and the loss function is not strictly convex. This means there is a possibility that gradient descent might get stuck in a local minimum rather than reaching the true values.\n",
    "\n",
    "<span style=\"color:red;\">In general, will your algorithm converge to the correct values irrespective of the starting point\n",
    "and irrespective of the examples?</span> No, the algorithm will not always converge to the correct values for all starting points and all datasets.\n",
    "\n",
    "Cubic functions are not linear. Gradient descent works best for convex functions, such as the Mean Squared Error (MSE) loss in linear regression. However, a cubic function is nonlinear, and its loss function can have multiple local minima.\n",
    "If we start from a poor initialization, gradient descent might get stuck in a local minimum rather than finding the correct values.\n",
    "\n",
    "As mentioned earlier, if the learning rate is too large, the updates might overshoot, causing divergence. If the learning rate is too small, it might take an excessive number of iterations to reach the true values.\n",
    "Finding the optimal learning rate is not always easy, especially when the function is cubic.\n",
    "\n",
    "The dataset we used only has four data points, which is too small to generalize well. If we used a different dataset with more varied $x$-values, the gradients would be different, and the algorithm might behave unpredictably. The success of gradient descent depends on having enough representative data that properly captures the cubic trend.\n",
    "\n",
    "If the starting values of $a,b,c,d$ are too far from the true values, gradient descent might take a long time to converge. If the loss function is highly non-convex, the algorithm may never reach the true values.\n",
    "\n",
    "If the input values of $x$ are large, the gradients can become large, making updates unstable. In this case, feature scaling the $x$-values would be a common way to improve convergence, but it is not always done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning the function with the given three algorithms\n",
    "\n",
    "<span style=\"color:red;\">Is there a way to learn the original function by some combination of the given three algorithms?</span>  $\\\\$ Yes, we can learn the function $y = ax^2 + bx + c$ by training all three algorithms separately and combining their outputs.\n",
    "\n",
    "We would train Algorithm1 to learn a function of the form of $y = ax^2$, which is a quadratic term. Use the dataset where $x$ is given and fit the function $y=ax^2$ using A1. This will give us an estimate of $a$, let's call it $\\hat{a}. \\\\$\n",
    "\n",
    "Then we would use the same dataset but instead fit the function $y=bx$ using Algorithm2. This will estimate $b$, desnoted as $\\hat{b}. \\\\$\n",
    "\n",
    "Use the dataset to fit the function $y=c$ using Algorithm3.\n",
    "This will estimate c, denoted as $\\hat{c}. \\\\$\n",
    "\n",
    "Once we obtain estimates $\\hat{a}, \\hat{b}, \\hat{c}$, the final prediction for any input $x$ can be computed as $\\hat{y} = \\hat{a}x^2 + \\hat{b}x + \\hat{c}$. Thus, by separately fitting each term and summing the predictions, we can reconstruct the full quadratic function.\n",
    "\n",
    "<span style=\"color:red;\">Does This Work for All Datasets?</span> Yes, the method works for any dataset because each of the three algorithms estimates one independent term of the quadratic function.\n",
    "Since polynomial regression is additive in its terms, we can train each term separately and sum their results.\n",
    "Linear regression ensures that each coefficient estimate is optimal in the least-squares sense.\n",
    "Thus, we can always reconstruct $y = ax^2 + bx + c$ as long as we apply all three models and sum their predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fitting the three line streaks with RANSAC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
